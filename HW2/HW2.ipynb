{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYXqkYe4SLC1"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UFQXqNLj6-2"
   },
   "source": [
    "ТЕКСТЫ. Сложность в омоформах, названиях и именях похожих на сущ. и тд."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbexhwIOSLC5"
   },
   "outputs": [],
   "source": [
    "text1 = \"\"\"Сидя в баре на Маросейке, Виноградова пила джин и ела макарон.\n",
    "Я вошёл в землянку, кругом лежали пилы и плюшки.\n",
    "Я надел наушники на уши, шапку на голову и повесил лестницу.\n",
    "Моя мама была ученым физиком, но тем не менее она хорошо готовила супы и закуски.\n",
    "Уличный шум стих, Петя пришёл домой и лёг на печь.\n",
    "Вода стекла по трубе, поэтому в доме больше не осталось стекла.\n",
    "Три тёрку и тогда сможешь приготовить мой вкусный осенний суп.\n",
    "Вертел я этот СОП на большом вертеле, его придумали серобуромалиновые черти.\n",
    "Я лечу к моей возлюбленной, потому что окрылен крыльями любви и программирования.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "2clRI8AScwTh"
   },
   "outputs": [],
   "source": [
    "gold1 = ['VERB', 'PREP', 'NOUN', 'PREP', 'NOUN', 'PROPN', 'VERB', 'NOUN', 'CONJ', 'VERB', 'NOUN',\n",
    "         'PRON', 'VERB', 'PREP', 'NOUN', 'ADV', 'VERB', 'NOUN', 'CONJ', 'NOUN',\n",
    "         'PRON', 'VERB', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'VERB', 'NOUN',\n",
    "         'PRON', 'NOUN', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'PRON', 'PART', 'ADV', 'PRON', 'ADV', 'VERB', 'NOUN', 'CONJ', 'NOUN',\n",
    "         'ADJ', 'NOUN', 'VERB', 'PROPN', 'VERB', 'NOUN', 'CONJ', 'VERB', 'PREP', 'NOUN',\n",
    "         'NOUN', 'VERB', 'PREP', 'NOUN', 'CONJ', 'PREP', 'NOUN', 'ADV', 'PART', 'VERB', 'NOUN',\n",
    "         'VERB', 'NOUN', 'CONJ', 'ADV', 'VERB', 'VERB', 'PRON', 'ADJ', 'ADJ', 'NOUN',\n",
    "         'VERB', 'PRON', 'PRON', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ', 'NOUN',\n",
    "         'PRON', 'VERB', 'PREP', 'PRON', 'NOUN', 'CONJ', 'CONJ', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'NOUN'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vkji16O3SLC9"
   },
   "outputs": [],
   "source": [
    "text2 = \"\"\"The black moon rose from behind the blind mountains.\n",
    "Johnson fired the match.\n",
    "A close look to the import informed us that the lead was broken.\n",
    "The object was refused because of his wound.\n",
    "The town authorities found money for the recreation of an ancient church.\n",
    "He decided to go to a resort for recreation.\n",
    "I will present him with this book.\n",
    "Her room is minute. There is a bed only. \n",
    "Your access is invalid.\n",
    "The diagram shows us a sharp increase in sales.\n",
    "He spoke with a grave tone which sounded weird.\n",
    "We sent four delegates to the conference. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tag7G9J6cy_n"
   },
   "outputs": [],
   "source": [
    "gold2 = ['DET', 'ADJ', 'NOUN', 'VERB', 'ADP', 'ADP', 'DET', 'ADJ', 'NOUN',\n",
    "         'PROPN', 'VERB', 'DET', 'NOUN',\n",
    "         'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'VERB', 'PRON', 'SCONJ', 'DET', 'NOUN', 'AUX', 'VERB',\n",
    "         'DET', 'NOUN', 'AUX', 'VERB', 'SCONJ', 'ADP', 'PRON', 'NOUN',\n",
    "         'DET', 'ADJ', 'NOUN', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN',\n",
    "         'PRON', 'VERB', 'PART', 'VERB', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN',\n",
    "         'PRON', 'AUX', 'VERB', 'PRON', 'ADP', 'DET', 'NOUN',\n",
    "         'PRON', 'NOUN', 'VERB', 'ADJ', 'PRON', 'VERB', 'DET', 'NOUN', 'ADV',\n",
    "         'DET', 'NOUN', 'VERB', 'ADJ',\n",
    "         'DET', 'NOUN', 'VERB', 'PRON', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN',\n",
    "         'PRON', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ',\n",
    "         'PRON', 'VERB', 'NUM', 'NOUN', 'ADP', 'DET', 'NOUN'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2hlS_y21nv3"
   },
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sFBINEQ2J8e"
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vDgI2ja2Uix"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jmYrv5G2ocV"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_eng = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text2)\n",
    "\n",
    "for i, s in enumerate(doc.sents):\n",
    "    for t in s:\n",
    "      if t.pos_ != 'SPACE' and t.pos_ != 'PUNCT':\n",
    "        spacy_eng.append(t.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYxx70o1SLDA"
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mY2gBm_HSLDA"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfWZTGKxSLDE"
   },
   "outputs": [],
   "source": [
    "tokens = [w.lower() for w in word_tokenize(text2) if w.isalpha()]\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "nltk_eng = []\n",
    "for word, tag in tagged:\n",
    "    nltk_eng.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2DCwNThSLDG"
   },
   "source": [
    "### FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEH9BMh3HyNu"
   },
   "outputs": [],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd3WazwcSLDI"
   },
   "outputs": [],
   "source": [
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzDkaB6ISLDG"
   },
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.tokenization import SegtokSentenceSplitter\n",
    "\n",
    "splitter = SegtokSentenceSplitter()\n",
    "\n",
    "sentences = splitter.split(text2)\n",
    "tagger = SequenceTagger.load('upos')\n",
    "tagger.predict(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRroLb55SLDP"
   },
   "outputs": [],
   "source": [
    "flair_eng = []\n",
    "for sentence in sentences:\n",
    "    list_of_tags = sentence.to_tagged_string().split()\n",
    "    for word in list_of_tags:\n",
    "      if '<' in word and word != '<PUNCT>':\n",
    "        word = re.sub('<|>', '', word)\n",
    "        flair_eng.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhuTCxMIDAVP"
   },
   "source": [
    "###### Тут я посмотрю что нашли мои парсеры и приведу к единой системе. За основу ляжет flair. Поскольку англ делался через колаб, некоторые аутпуты могут быть пустыми, но ничего важного в них нет, просто удобство"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "fV_djLhDSLDT",
    "outputId": "38ef37d4-a8b5-4e2a-efba-dbef4f444672"
   },
   "outputs": [],
   "source": [
    "p = Counter(flair_eng)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XAfIEZbbrrsY",
    "outputId": "963beb0a-f7d1-480a-a153-57c1c9a34a7b"
   },
   "outputs": [],
   "source": [
    "o = Counter(spacy_eng)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eAnYlag0zqQp",
    "outputId": "f45951d7-c325-49fa-dfe6-27861b940c9c"
   },
   "outputs": [],
   "source": [
    "q = Counter(nltk_eng)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTagy84D0rCM"
   },
   "outputs": [],
   "source": [
    "#ещё не совсем ясно, считать ли кучу всего adpositions, или разделять на предлоги и т.п. Я считал это все одниим, потому что у двух теггеров просто нет такой категории как предлог.\n",
    "#nltk\n",
    "\"NN\" = \"NOUN\"\n",
    "\"DT\" = \"DET\"\n",
    "# тут не очень понятно что делать, потому что нлтк размечает и предлоги, и союзы как in. Поэтому будет все adposition\n",
    "\"IN\" = \"ADP\"\n",
    "\"VBD\" = \"VERB\"\n",
    "\"JJ\" = \"ADJ\"\n",
    "\"PRP\" = \"PRON\"\n",
    "\"VBZ\" = \"VERB\"\n",
    "\"NNS\" = \"NOUN\"\n",
    "#тут тоже непонятно, потому что другие парсеры делят to на adposition and particle. Но большинство случаев adp.\n",
    "\"TO\" = \"ADP\"\n",
    "\"PRP$\" = \"PRON\"\n",
    "\"VBN\" = \"VERB\"\n",
    "\"VB\" = \"VERB\"\n",
    "\"MD\" = \"VERB\"\n",
    "\"EX\" = \"PRON\"\n",
    "\"RB\" = \"ADV\"\n",
    "\"EX\" = \"PRON\"\n",
    "\"WDT\" = \"DET\"\n",
    "\"CD\" = \"NUM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zXrgchfX8WMC"
   },
   "outputs": [],
   "source": [
    "nltk_st = []\n",
    "for tag in nltk_eng:\n",
    "  if tag == 'NN':\n",
    "    tag = re.sub('NN', 'NOUN', tag)\n",
    "\n",
    "  elif tag == 'DT':\n",
    "    tag = re.sub('DT', 'DET', tag)\n",
    "\n",
    "  elif tag == 'IN':\n",
    "    tag = re.sub('IN', 'ADP', tag)\n",
    "\n",
    "  elif tag == 'VBD':\n",
    "    tag = re.sub('VBD', 'VERB', tag)\n",
    "\n",
    "  elif tag == 'JJ':\n",
    "    tag = re.sub('JJ', 'ADJ', tag)\n",
    "\n",
    "  elif tag == 'PRP':\n",
    "    tag = re.sub('PRP', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'VBZ':\n",
    "    tag = re.sub('VBZ', 'VERB', tag)\n",
    "\n",
    "  elif tag == 'NNS':\n",
    "    tag = re.sub('NNS', 'NOUN', tag)\n",
    "\n",
    "  elif tag == 'TO':\n",
    "    tag = re.sub('TO', 'ADP', tag)\n",
    "\n",
    "  elif tag == 'PRP$':\n",
    "    tag = re.sub(r'PRP.+', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'VBN':\n",
    "    tag = re.sub('VBN', 'VERB', tag)\n",
    "  \n",
    "  elif tag == 'VB':\n",
    "    tag = re.sub('VB', 'VERB', tag)\n",
    "  \n",
    "  elif tag == 'MD':\n",
    "    tag = re.sub('MD', 'VERB', tag)\n",
    "\n",
    "  elif tag == 'EX':\n",
    "    tag = re.sub('EX', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'RB':\n",
    "    tag = re.sub('RB', 'ADV', tag)\n",
    "\n",
    "  elif tag == 'EX':\n",
    "    tag = re.sub('EX', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'WDT':\n",
    "    tag = re.sub('WDT', 'DET', tag)\n",
    " \n",
    "  elif tag == 'CD':\n",
    "    tag = re.sub('CD', 'NUM', tag)\n",
    "  nltk_st.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "CoDu3Dzs8WKH",
    "outputId": "886074e2-17e5-4745-b59e-3f8d89ae0c32"
   },
   "source": [
    "И считаем accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZoGekEb4kjq"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "TlmJCIK54leg",
    "outputId": "353a73dc-29f2-4ab8-ef42-d10c8496f569"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Spacy accuracy: ' + str(accuracy_score(spacy_eng, gold2)))\n",
    "print('Flair accuracy: ' + str(accuracy_score(flair_eng, gold2)))\n",
    "print('NLTK accuracy: ' + str(accuracy_score(nltk_st, gold2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJJmWeb-SLDV"
   },
   "source": [
    "### MYSTEM\n",
    "Чтобы быстрее работало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohPQQA28SLDW"
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "lemmas = m.analyze(text1)\n",
    "mystem_ru = []\n",
    "for word in lemmas:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        mystem_ru.append(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foyDagMzSLDY"
   },
   "source": [
    "### PYMORHPY\n",
    "Можно учесть вероятности разборов определенной части речи, или посмотреть не попал ли какой-то разбор в верное, но я не буду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kwz6nDySLDZ"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()\n",
    "pymorphy_ru = []\n",
    "words = [w.lower() for w in word_tokenize(text1) if w.isalpha()]\n",
    "for word in words:\n",
    "    x = morph.parse(word)\n",
    "    POS = x[0].tag.POS\n",
    "    pymorphy_ru.append(POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaA3SsaTSLDb"
   },
   "source": [
    "### NATASHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMgS4S8uSLDb"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkjg_GuoSLDd"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_V_pP7rnSLDf"
   },
   "outputs": [],
   "source": [
    "doc = Doc(text1)\n",
    "doc.segment(segmenter)\n",
    "natasha_ru = []\n",
    "doc.tag_morph(morph_tagger)\n",
    "for token in doc.tokens:\n",
    "    if token.pos != 'PUNCT':\n",
    "        natasha_ru.append(token.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SesOFbQ-SLDh"
   },
   "source": [
    "###### Приводим все к одному"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78bGdPpMSLDj"
   },
   "outputs": [],
   "source": [
    "#natasha - standart\n",
    "#mystem -> natasha\n",
    "'V' = 'VERB'\n",
    "'S' = 'NOUN'\n",
    "'PR' = 'PREP'\n",
    "'A' = 'ADJ'\n",
    "'APRO' = 'PRON'\n",
    "'ADVPRO' = 'ADV' #местоименное наречие привожу к обычному\n",
    "'SPRO' = 'PRON'\n",
    "\n",
    "#natasha -> mystem\n",
    "#тут нельзя разделить адпозишн на частицу и предлог, но чаще это предлоги, так что буду считать их\n",
    "'ADP' = 'PREP'\n",
    "'SCONJ' = 'CONJ'\n",
    "'CCONJ' = 'CONJ'\n",
    "\n",
    "#pymorhpy -> natasha\n",
    "'ADVB' = 'ADV'\n",
    "'ADJF' = 'ADJ'\n",
    "'NPRO' = 'PRON'\n",
    "'PRCL' = 'PART'\n",
    "'COMP' = 'ADJ' #сравнение может быть как прил, так и наречием, но прилагательные чаще\n",
    "'NUMR' = 'NUM'\n",
    "'INFN' = 'VERB'\n",
    "#причастия я не трогаю, потому что они не встретились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem_st = []\n",
    "for tag in mystem_ru:\n",
    "  if tag == 'V':\n",
    "    tag = re.sub('V', 'VERB', tag)\n",
    "\n",
    "  elif tag == 'S':\n",
    "    tag = re.sub('S', 'NOUN', tag)\n",
    "\n",
    "  elif tag == 'PR':\n",
    "    tag = re.sub('PR', 'PREP', tag)\n",
    "    \n",
    "  elif tag == 'A':\n",
    "    tag = re.sub('A', 'ADJ', tag)\n",
    "    \n",
    "  elif tag == 'APRO':\n",
    "    tag = re.sub('APRO', 'PRON', tag)\n",
    "    \n",
    "  elif tag == 'ADVPRO':\n",
    "    tag = re.sub('ADVPRO', 'ADV', tag)\n",
    "    \n",
    "  elif tag == 'SPRO':\n",
    "    tag = re.sub('SPRO', 'PRON', tag)\n",
    "    \n",
    "  mystem_st.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwlIi2hJSLDt"
   },
   "outputs": [],
   "source": [
    "natasha_st = []\n",
    "for tag in natasha_ru:\n",
    "  if tag == 'ADP':\n",
    "    tag = re.sub('ADP', 'PREP', tag)\n",
    "\n",
    "  elif tag == 'SCONJ':\n",
    "    tag = re.sub('SCONJ', 'CONJ', tag)\n",
    "\n",
    "  elif tag == 'CCONJ':\n",
    "    tag = re.sub('CCONJ', 'CONJ', tag)\n",
    "  \n",
    "  natasha_st.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUtqwlF0SLDm"
   },
   "outputs": [],
   "source": [
    "pymorhpy_st = []\n",
    "for tag in pymorphy_ru:\n",
    "  if tag == 'ADVB':\n",
    "    tag = re.sub('ADVB', 'ADV', tag)\n",
    "\n",
    "  elif tag == 'PRTF':\n",
    "    tag = re.sub('PRTF', 'VERB', tag)\n",
    "\n",
    "  elif tag == 'PRTS':\n",
    "    tag = re.sub('PRTS', 'VERB', tag)\n",
    "    \n",
    "  elif tag == 'ADJF':\n",
    "    tag = re.sub('ADJF', 'ADJ', tag)\n",
    "\n",
    "  elif tag == 'NPRO':\n",
    "    tag = re.sub('NPRO', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'PRCL':\n",
    "    tag = re.sub('PRCL', 'PART', tag)\n",
    "\n",
    "  elif tag == 'NPRO':\n",
    "    tag = re.sub('NPRO', 'PRON', tag)\n",
    "\n",
    "  elif tag == 'COMP':\n",
    "    tag = re.sub('COMP', 'ADJ', tag)\n",
    "\n",
    "  elif tag == 'NUMR':\n",
    "    tag = re.sub('NUMR', 'NUM', tag)\n",
    "\n",
    "  elif tag == 'INFN':\n",
    "    tag = re.sub('INFN', 'VERB', tag)\n",
    "    \n",
    "  pymorhpy_st.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считает accuracy для русского, майстем победил"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Pn5FHRYOSLDq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mystem accuracy: 0.9\n",
      "Natasha accuracy: 0.85\n",
      "Pymorphy accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "print('Mystem accuracy: ' + str(accuracy_score(mystem_st, gold1)))\n",
    "print('Natasha accuracy: ' + str(accuracy_score(natasha_st, gold1)))\n",
    "print('Pymorphy accuracy: ' + str(accuracy_score(pymorhpy_st, gold1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A_kZ7jFSLDl"
   },
   "source": [
    "### Часть два"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'PREP', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'NOUN', 'CONJ', 'VERB', 'NOUN', 'PRON', 'VERB', 'PREP', 'NOUN', 'ADV', 'VERB', 'NOUN', 'CONJ', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PREP', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'CONJ', 'VERB', 'NOUN', 'PRON', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'PRON', 'PART', 'ADV', 'PRON', 'ADV', 'VERB', 'NOUN', 'CONJ', 'NOUN', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'ADV', 'CONJ', 'VERB', 'PREP', 'NOUN', 'NOUN', 'NOUN', 'PREP', 'NOUN', 'ADV', 'PREP', 'NOUN', 'ADV', 'PART', 'VERB', 'NOUN', 'NUM', 'NOUN', 'CONJ', 'ADV', 'VERB', 'VERB', 'PRON', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'PRON', 'PRON', 'NOUN', 'PREP', 'ADJ', 'NOUN', 'PRON', 'VERB', 'ADJ', 'NOUN', 'PRON', 'VERB', 'PREP', 'PRON', 'NOUN', 'ADV', 'CONJ', 'VERB', 'NOUN', 'NOUN', 'CONJ', 'NOUN']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересные сочетания: прил+сущ, не+глагол, сущ+наречие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigramm_maker(text):\n",
    "    adj_noun = []\n",
    "    ne_verb = []\n",
    "    noun_adv = []\n",
    "    m = Mystem()\n",
    "    lemmas = m.analyze(text1)\n",
    "    for word in lemmas:\n",
    "        if 'analysis' in word:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            mystem_ru.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramm_maker(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "lemmas = m.analyze(text1)\n",
    "mystem_ru = []\n",
    "for word in lemmas:\n",
    "    if 'analysis' in word:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        mystem_ru.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
